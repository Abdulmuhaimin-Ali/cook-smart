```markdown
## Purpose & Overview

This `server/server.js` file creates an Express.js server that exposes a single API endpoint (`/recipeStream`) to generate meal recipes based on user preferences using the OpenAI API. The server utilizes Server-Sent Events (SSE) to stream the recipe generation result back to the client in real-time. This allows the client to display the recipe as it's being generated, improving the user experience.

## Key Functions/Components

*   **Express.js:** The web framework used to create the server and define the API endpoint.
*   **CORS:** Middleware to enable Cross-Origin Resource Sharing, allowing requests from different domains (e.g., a frontend running on `localhost:3000` requesting data from the backend on `localhost:3001`).
*   **OpenAI API:**  Connects the server to the OpenAI API, specifically the `chat.completions` endpoint, for generating the recipe content.
*   **dotenv:** Loads environment variables from a `.env` file (specifically the `OPENAI_API_KEY`).
*   **`/recipeStream` endpoint:** The main API endpoint that receives user preferences as query parameters, constructs a prompt for the OpenAI API, calls the OpenAI API, and streams the response back to the client using SSE.
*   **`fetchOpenAiCompletionStream(messages, callback)` Function:**  This asynchronous function handles the communication with the OpenAI API. It takes a list of messages (forming the prompt) and a callback function.  It calls the OpenAI API's `chat.completions.create` method with the `stream: true` option, receives the response as a stream of chunks, and passes each chunk to the provided callback function.
*   **`sendEvent(chunk)` Function:** This function processes each chunk of the OpenAI response and formats it into an SSE event.  It determines the action to be taken by the client (start, chunk, or close) and sends the appropriate data in the correct SSE format.

## Business Logic

The core business logic revolves around:

1.  **Receiving User Preferences:** The `/recipeStream` endpoint accepts user preferences (meal type, cuisine, dietary concerns, cooking time, servings, and target calories) as query parameters.
2.  **Constructing the OpenAI Prompt:** These preferences are then used to build a detailed prompt for the OpenAI API, requesting a recipe tailored to the user's specifications.  The prompt includes instructions on formatting the output clearly with a title, ingredients, and step-by-step instructions.
3.  **Calling the OpenAI API:** The `fetchOpenAiCompletionStream` function uses the OpenAI `chat.completions.create` method with the `stream: true` option to retrieve the generated recipe in chunks.
4.  **Streaming the Response:** The server uses Server-Sent Events (SSE) to stream the recipe content to the client as it's being generated by the OpenAI API.  The `sendEvent` function handles formatting the data into SSE events.
5.  **Error Handling:** The `fetchOpenAiCompletionStream` function includes a `try...catch` block to handle potential errors from the OpenAI API and log them to the console.

## Input/Output Specifications

*   **Input (to `/recipeStream` endpoint):**
    *   Query parameters:
        *   `mealType` (string): Type of meal (e.g., breakfast, lunch, dinner).
        *   `cuisine` (string):  Cuisine of the meal (e.g., Italian, Mexican, Indian).
        *   `dietConcerns` (string): Dietary restrictions or concerns (e.g., vegetarian, gluten-free, vegan).
        *   `cookingTime` (string): Maximum cooking time in minutes.
        *   `servings` (string): Number of servings.
        *   `targetCalories` (string): Target calories per serving.

*   **Output (from `/recipeStream` endpoint):**
    *   Server-Sent Events (SSE) stream with the following data format:
        ```json
        data: {"action": "start"}

        data: {"action": "chunk", "chunk": "This is the first chunk of the recipe."}

        data: {"action": "chunk", "chunk": "This is another chunk of the recipe."}

        data: {"action": "close"}
        ```
        *   `action`: Specifies the action to be taken by the client.
            *   `start`: Indicates the beginning of the recipe generation.
            *   `chunk`: Contains a portion of the recipe text.
            *   `close`: Indicates the completion of the recipe generation.
        *   `chunk`: (Only present when `action` is "chunk") The actual text content of the recipe chunk.

## Usage Examples

To use the API, send a GET request to the `/recipeStream` endpoint with the required query parameters:

```
GET http://localhost:3001/recipeStream?mealType=dinner&cuisine=Italian&dietConcerns=vegetarian&cookingTime=30&servings=2&targetCalories=500
```

The client should then listen for Server-Sent Events from this endpoint and process the data based on the `action` field. For example, the client's javascript would create an EventSource object and listen for 'message' events:

```javascript
const eventSource = new EventSource('http://localhost:3001/recipeStream?mealType=dinner&cuisine=Italian&dietConcerns=vegetarian&cookingTime=30&servings=2&targetCalories=500');

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);

  if (data.action === 'start') {
    console.log('Recipe generation started.');
  } else if (data.action === 'chunk') {
    console.log('Received chunk:', data.chunk);
    // Append the chunk to a display element on the page
  } else if (data.action === 'close') {
    console.log('Recipe generation complete.');
    eventSource.close(); // close the connection
  }
};

eventSource.onerror = (error) => {
    console.error("EventSource failed:", error);
    eventSource.close();
}
```

## Dependencies

*   **express:**  Web framework for Node.js.
*   **cors:** Middleware for enabling Cross-Origin Resource Sharing.
*   **openai:** OpenAI API client library for Node.js.
*   **dotenv:**  Module for loading environment variables from a `.env` file.

## Important Notes

*   **API Key Security:**  The `OPENAI_API_KEY` is loaded from the `.env` file.  **Do not hardcode your API key directly into the code.**  Always use environment variables for sensitive information.
*   **Error Handling:**  The code includes basic error handling for the OpenAI API call.  Robust error handling should be implemented throughout the application, including logging and appropriate responses to the client.
*   **Rate Limiting:** The OpenAI API has rate limits.  Implement rate limiting on the server-side to avoid exceeding the limits and causing errors.
*   **Prompt Engineering:** The quality of the generated recipes depends heavily on the prompt provided to the OpenAI API.  Experiment with different prompt structures and wording to optimize the results.  Consider adding more context or specific instructions to improve the relevance and accuracy of the generated recipes.  For example, you could provide example output formats in the prompt.
*   **Model Selection:** The code uses `gpt-5-mini` as the AI model. Ensure that the specified model name is valid and accessible through your OpenAI API account.  Consider experimenting with different models to find the best balance between cost and performance.
*   **Data Validation:** Validate the input query parameters to prevent errors and ensure that the prompt is constructed correctly. Sanitize the inputs to prevent prompt injection attacks.
*   **SSE Connection Management:**  The `req.on('close', ...)` ensures that the SSE connection is properly closed when the client disconnects. This prevents resource leaks on the server.
*   **Streaming Considerations:** The streaming response is formatted with newline characters (`\n\n`) to adhere to the SSE specification. Ensure that your client-side code correctly parses these events.
*   **Scalability:** For a high-traffic application, consider using a message queue or other asynchronous processing mechanism to handle OpenAI API requests in the background. This can improve the responsiveness of the server and prevent it from becoming overloaded.
