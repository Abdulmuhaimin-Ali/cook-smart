```markdown
## Purpose & Overview

This `server.js` file creates an Express server that provides a streaming API endpoint (`/recipeStream`) for generating recipes using the OpenAI API. The server receives recipe preferences as query parameters, constructs a prompt for the OpenAI API, and streams the recipe generation back to the client using Server-Sent Events (SSE).  The server also handles environment variable configuration via `dotenv`.

## Key Functions/Components

*   **Express Server:**  Handles incoming HTTP requests and responses.
*   **CORS Middleware:** Enables Cross-Origin Resource Sharing, allowing the frontend to make requests to the server running on a different origin.
*   **`/recipeStream` Endpoint:**  The primary API endpoint that receives recipe preferences and streams the generated recipe.
*   **`fetchOpenAiCompletionStream` Function:**  Handles the interaction with the OpenAI API. It creates a completion stream using the OpenAI Node.js library and passes each chunk of data to a callback function.
*   **SSE (Server-Sent Events):**  The mechanism used for streaming the recipe generation from the server to the client.
*   **Environment Variable Configuration:** Loads environment variables such as the OpenAI API key from a `.env` file.
*   **OpenAI API Integration:** Utilizes the OpenAI Node.js library to interact with the OpenAI chat API for recipe generation.

## Business Logic (if applicable)

The business logic revolves around generating a recipe based on user preferences communicated as query parameters to the `/recipeStream` endpoint.

1.  **Receive and Parse Request:** The server receives the HTTP request to `/recipeStream` and extracts recipe parameters (meal type, cuisine, dietary concerns, cooking time, servings, target calories) from the query string.
2.  **Construct OpenAI Prompt:** It creates a prompt tailored to the OpenAI API based on these parameters. The prompt guides the AI to generate a recipe with a specific format (title, ingredients, instructions).
3.  **OpenAI API Call:** The server then calls the `fetchOpenAiCompletionStream` function with the prepared prompt.
4.  **Stream Recipe Generation:** The `fetchOpenAiCompletionStream` function initiates a streaming request to the OpenAI API. As the OpenAI API generates the recipe, it sends back chunks of text.
5.  **SSE Formatting and Transmission:** The `sendEvent` function formats each chunk into an SSE message and sends it to the client.  The SSE format includes a `data:` prefix and a double newline (`\n\n`) to delimit events. The function also handles sending a closing event `action: "close"` when the stream from OpenAI finishes.
6.  **Client Disconnect Handling:** The server listens for the `close` event on the request object (`req.on("close", ...)`). When the client disconnects, it calls `res.end()` to terminate the SSE connection and prevent resource leaks.

## Input/Output Specifications

**Input (`/recipeStream` endpoint):**

*   **HTTP Method:** `GET`
*   **Query Parameters:**
    *   `mealType`: String (e.g., "Breakfast", "Lunch", "Dinner")
    *   `cuisine`: String (e.g., "Italian", "Mexican", "Indian")
    *   `dietConcerns`:  String (e.g., "Vegetarian", "Vegan", "Gluten-Free")
    *   `cookingTime`: Number (in minutes, e.g., "30")
    *   `servings`: Number (e.g., "2")
    *   `targetCalories`: Number (calories per serving, e.g., "500")

**Output (`/recipeStream` endpoint):**

*   **HTTP Status:** `200 OK`
*   **Content-Type:** `text/event-stream`
*   **Body:** A stream of Server-Sent Events (SSE). Each event contains a JSON object as data.  The JSON object has the following structure:

    *   `{ action: "start" }`: Sent at the beginning of the recipe generation.  The first chunk from the response will trigger this.
    *   `{ action: "chunk", chunk: "some recipe text" }`:  Sent for each chunk of the recipe generated by the OpenAI API.  `chunk` contains a portion of the generated recipe (string).
    *   `{ action: "close" }`: Sent when the recipe generation is complete.

**Example SSE Message:**

```
data: {"action":"chunk","chunk":"Preheat oven to 375°F (190°C)."}

```

## Usage Examples

**Example Client-Side JavaScript (using EventSource):**

```javascript
const eventSource = new EventSource(
  `http://localhost:3001/recipeStream?mealType=Dinner&cuisine=Italian&dietConcerns=Vegetarian&cookingTime=45&servings=2&targetCalories=600`
);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.action === "start") {
    console.log("Recipe generation started!");
  } else if (data.action === "chunk") {
    console.log(data.chunk); // Print each chunk of the recipe
  } else if (data.action === "close") {
    console.log("Recipe generation complete!");
    eventSource.close(); // Close the SSE connection
  }
};

eventSource.onerror = (error) => {
  console.error("EventSource failed:", error);
  eventSource.close();
};
```

This client-side code establishes an SSE connection to the `/recipeStream` endpoint with the specified query parameters. It then listens for `message` events.  Each event's data is parsed as JSON. The script prints the recipe chunks to the console and closes the connection when the "close" event is received.

## Dependencies

*   **express:**  Web application framework for Node.js. `npm install express`
*   **cors:**  Middleware for enabling Cross-Origin Resource Sharing (CORS). `npm install cors`
*   **openai:**  OpenAI Node.js library for interacting with the OpenAI API. `npm install openai`
*   **dotenv:**  Loads environment variables from a `.env` file. `npm install dotenv`

## Important Notes

*   **API Key Security:** The OpenAI API key should be stored securely as an environment variable and **never** committed directly to the repository.  Make sure `.env` is in your `.gitignore` file.
*   **Error Handling:** The `fetchOpenAiCompletionStream` function includes a basic `try...catch` block for error handling.  More robust error handling should be implemented to handle potential issues with the OpenAI API, network connectivity, and other unexpected errors.  The errors should be communicated to the client.
*   **Model Selection:** The `aiModel` variable currently uses `"gpt-5-mini"`. Ensure this model identifier is valid and available within your OpenAI account.  Consider making this configurable through an environment variable.
*   **Rate Limiting:** Be aware of OpenAI API rate limits. Implement appropriate retry mechanisms and error handling to gracefully handle rate limiting errors.
*   **Prompt Engineering:**  The prompt used to interact with the OpenAI API is critical for generating desired recipe results. Experiment with different prompt variations to optimize the quality and format of the generated recipes.
*   **Streaming Efficiency:**  While Server-Sent Events are used, large recipes might benefit from additional chunking or buffering strategies on both the server and client to optimize streaming efficiency.
*   **Content Filtering:** It is essential to implement content filtering mechanisms to prevent the generation of inappropriate or harmful content by the OpenAI API.
*   **Client Disconnects:**  The `req.on("close", ...)` handler is crucial for cleaning up resources when a client disconnects prematurely. Without this, the server might continue streaming data to a non-existent client, potentially leading to resource exhaustion.
*   **Data Validation:** Input validation should be implemented on the server to ensure that the query parameters are of the correct type and within acceptable ranges. This can help prevent errors and security vulnerabilities.
*   **Scalability:** For production environments, consider using a more robust streaming framework or message queue system (e.g., Kafka, RabbitMQ) to handle a high volume of concurrent requests and ensure scalability.
*   **Security:** Implement appropriate security measures to protect the API endpoint, such as authentication and authorization, especially if the API is exposed publicly.
```